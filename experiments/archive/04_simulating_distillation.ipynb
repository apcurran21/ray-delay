{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: gather statistics on average overhead of a cosmic ray, based on the detection statistics generated in the previous notebook. Also, experiment with adding redundancy in the factory.\n",
    "\n",
    "### Note: much of the code written here is now being implemented in `/ray_delay/factory_simulator.py` for easier use in further experiments. The code in this file is not necessarily up-to-date with the newer versions in `/ray_delay/factory_simulator.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import scipy\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import mpmath\n",
    "import dill\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from ray_delay.ray_detector import RayDetectorSpec\n",
    "from stim_surface_code import patch\n",
    "\n",
    "import qc_utils.stats\n",
    "\n",
    "mpmath.mp.dps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the high-accuracy simulations in notebooks 02-05, we are now zooming out\n",
    "to look at algorithm-level performance. To do this, we must abandon Stim\n",
    "simulations and instead adopt a simpler model of cosmic rays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx(l):\n",
    "    return sum(x << i for i, x in enumerate(reversed(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([[0,1,2],[3,4,5],[6,7,8]])\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagicStateFactory:\n",
    "    \"\"\"TODO\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            dx: int = 7,\n",
    "            dz: int = 7,\n",
    "            dm: int = 7,\n",
    "            patch_offline_duration: float = 30e-3,\n",
    "            cycle_time: float | None = None,\n",
    "            cache_cycles_per_distillation: bool = False,\n",
    "            cache_filename: str | None = None,\n",
    "        ):\n",
    "        \"\"\"Initializes the factory.\n",
    "\n",
    "        Args:\n",
    "            dx: X code distance for each patch.\n",
    "            dz: Z code distance for each patch.\n",
    "            dm: Temporal code distance.\n",
    "            patch_offline_duration: Amount of time (in seconds) a patch is taken\n",
    "                offline when a ray is detected.\n",
    "            cycle_time: Duration of a single surface code stabilizer measurement\n",
    "                cycle. If None, will be calculated based on the code distance.\n",
    "            cache_cycles_per_distillation: If True, store results of previous\n",
    "                calls to _cycles_per_distillation() to avoid redundant\n",
    "                calculations.\n",
    "            cache_filename: Filename to use for caching solutions to \n",
    "                _cycles_per_distillation(). If None, will not load or save\n",
    "                cache.\n",
    "        \"\"\"\n",
    "        self.dx = dx\n",
    "        self.dz = dz\n",
    "        self.dm = dm\n",
    "        self.patch_offline_duration = patch_offline_duration\n",
    "\n",
    "        num_rows = 3\n",
    "        num_cols = 5\n",
    "        row_heights = np.full(num_rows, self.dx)\n",
    "        col_widths = np.array([self.dx] + [self.dz]*4)\n",
    "\n",
    "        self.num_patches = num_rows * num_cols\n",
    "        self.patch_indices = np.reshape(np.arange(self.num_patches), (num_rows, num_cols))\n",
    "        self.patch_coords_from_idx = np.zeros((self.num_patches, 2), int)\n",
    "        for idx in range(self.num_patches):\n",
    "            self.patch_coords_from_idx[idx] = [idx // self.patch_indices.shape[1], idx % self.patch_indices.shape[1]]\n",
    "\n",
    "        # use stim_surface_code.patch to generate the physical qubit array\n",
    "        # TODO: currently does not account for extra 4*dm space needed for magic\n",
    "        # state injection (see figs. 10-11 of Litinski). Is this important?\n",
    "        surface_code_patch = patch.SurfaceCodePatch(num_rows*dx + (num_rows-1), dx + 4*dz + (num_cols-1), dm)\n",
    "        self.physical_qubit_array = np.array([[(q.idx if q is not None else -1) for q in row] for row in surface_code_patch.device], int)\n",
    "        self.physical_qubit_coords_from_idx = np.zeros((len(surface_code_patch.all_qubits), 2), int)\n",
    "        for q in surface_code_patch.all_qubits:\n",
    "            self.physical_qubit_coords_from_idx[q.idx] = q.coords\n",
    "        if cycle_time is None:\n",
    "            self.cycle_time = surface_code_patch.cycle_time()\n",
    "        else:\n",
    "            self.cycle_time = cycle_time\n",
    "\n",
    "        # maps each physical qubit to a patch index\n",
    "        self.num_phys_qubits = len(surface_code_patch.all_qubits)\n",
    "        self.patch_idx_from_physical_qubit_idx = np.full(self.num_phys_qubits, -1, dtype=int)\n",
    "        for row in range(num_rows):\n",
    "            min_phys_row = np.sum(2*row_heights[:row]+1) + row\n",
    "            max_phys_row = min_phys_row + 2*row_heights[row]+1\n",
    "            for col in range(num_cols):\n",
    "                min_phys_col = np.sum(2*col_widths[:col]+1) + col\n",
    "                max_phys_col = min_phys_col + 2*col_widths[col]+1\n",
    "                for phys_row in range(min_phys_row, max_phys_row):\n",
    "                    for phys_col in range(min_phys_col, max_phys_col):\n",
    "                        phys_idx = self.physical_qubit_array[phys_row, phys_col]\n",
    "                        if phys_idx != -1:\n",
    "                            self.patch_idx_from_physical_qubit_idx[phys_idx] = self.patch_indices[row, col]\n",
    "\n",
    "        self.physical_qubit_offline_time_remaining = np.zeros(self.num_phys_qubits, float)\n",
    "\n",
    "        self.prev_patches_online = None\n",
    "        self.prev_distillation_cycles = None\n",
    "\n",
    "        self._cache_cycles_per_distillation = cache_cycles_per_distillation\n",
    "        self.cache_filename = cache_filename\n",
    "        self._cycles_per_distillation_cache = {}\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(\n",
    "            self,\n",
    "            cache_filename: str | None = None,\n",
    "        ):\n",
    "        \"\"\"Load the cache of _cycles_per_distillation() from a file. Does\n",
    "        nothing if the file does not exist.\n",
    "\n",
    "        Args:\n",
    "            cache_filename: Filename to load the cache from.\n",
    "        \"\"\"\n",
    "        if cache_filename is None:\n",
    "            if self.cache_filename is None:\n",
    "                return\n",
    "            cache_filename = self.cache_filename\n",
    "        if os.path.exists(cache_filename):\n",
    "            with open(cache_filename, 'rb') as f:\n",
    "                self._cycles_per_distillation_cache = pickle.load(f)\n",
    "\n",
    "    def save_cache(\n",
    "            self,\n",
    "            cache_filename: str | None = None,\n",
    "        ):\n",
    "        \"\"\"Save the cache of _cycles_per_distillation() to a file. Overwrites\n",
    "        any existing file.\n",
    "\n",
    "        Args:\n",
    "            cache_filename: Filename to save the cache to.\n",
    "        \"\"\"\n",
    "        if cache_filename is None:\n",
    "            if self.cache_filename is None:\n",
    "                return\n",
    "            cache_filename = self.cache_filename\n",
    "        with open(cache_filename, 'wb') as f:\n",
    "            pickle.dump(self._cycles_per_distillation_cache, f)\n",
    "\n",
    "    def calculate_avg_overhead_per_ray(\n",
    "            self,\n",
    "            ray_detector_spec: RayDetectorSpec,\n",
    "            num_rays: int,\n",
    "            num_concurrent_rays: int,\n",
    "            num_distributions: int = 1000,\n",
    "            prob_cutoff: float = 1e-7,\n",
    "        ): \n",
    "        \"\"\"Simulate the impact of a number of cosmic rays on the factory\n",
    "\n",
    "        TODO: rays centered just outside of factory?\n",
    "        \n",
    "        Args:\n",
    "            ray_detector_spec: Contains information about ray and detector\n",
    "                behavior.\n",
    "            num_rays: Number of ray impacts to simulate.\n",
    "            num_concurrent_rays: Number of rays to simulate at once.\n",
    "            num_distributions: Number of qubit detection distributions to sample\n",
    "                for each ray impact.\n",
    "            prob_cutoff: Minimum probability of a distribution for it to be\n",
    "                considered.\n",
    "            \n",
    "        Returns:\n",
    "            Average additional cycle cost of a ray impact.\n",
    "        \"\"\"\n",
    "        self._reset()\n",
    "        baseline_cycles = self._cycles_per_distillation()\n",
    "        assert baseline_cycles is not None\n",
    "        ray_results = self.simulate_ray_impacts(ray_detector_spec, num_rays)\n",
    "        time_overheads = []\n",
    "        offline_chances = []\n",
    "        # for each ray result, calculate expected (average case) patch offline time\n",
    "        for j,ray_result in enumerate(ray_results):\n",
    "            qubit_detection_chances = ray_result[1]\n",
    "\n",
    "            # qubits_offline_list, distribution_chances = qc_utils.stats.get_most_probable_bitstrings(qubit_detection_chances, 10, probability_threshold=1e-3)\n",
    "            qubits_offline_array = np.random.rand(num_distributions, len(qubit_detection_chances)) < qubit_detection_chances[None, :][[0]*num_distributions]\n",
    "            \n",
    "            time_overhead = 0.0\n",
    "            total_prob = 0\n",
    "            offline_chances.append(0.0)\n",
    "            for i,qubits_offline in enumerate(qubits_offline_array):\n",
    "                distribution_chance = np.prod(qubit_detection_chances[qubits_offline]) * np.prod(1-qubit_detection_chances[~qubits_offline])\n",
    "                if distribution_chance < prob_cutoff:\n",
    "                    continue\n",
    "                self.physical_qubit_offline_time_remaining = qubits_offline.astype(float)\n",
    "                distillation_cycles = self._cycles_per_distillation()\n",
    "                if distillation_cycles is None:\n",
    "                    offline_chances[-1] += distribution_chance\n",
    "                else:\n",
    "                    time_overhead += distillation_cycles / baseline_cycles * distribution_chance\n",
    "                total_prob += distribution_chance\n",
    "            working_total_prob = total_prob - offline_chances[-1]\n",
    "            if working_total_prob > 0:\n",
    "                time_overhead /= working_total_prob\n",
    "                time_overheads.append(time_overhead)\n",
    "            offline_chances[-1] /= total_prob\n",
    "        self._reset()\n",
    "        self.save_cache()\n",
    "\n",
    "        return np.mean(time_overheads), np.mean(offline_chances)\n",
    "    \n",
    "    def calculate_avg_overhead_one_patch_offline(\n",
    "            self,\n",
    "        ): \n",
    "        \"\"\"TODO\n",
    "            \n",
    "        Returns:\n",
    "            Average additional cycle cost when one patch is taken offline.\n",
    "        \"\"\"\n",
    "        self._reset()\n",
    "        baseline_distillation_cycles = self._cycles_per_distillation()\n",
    "        assert baseline_distillation_cycles is not None\n",
    "        time_overheads = []\n",
    "        offline_count = 0\n",
    "        # for each ray result, calculate expected (average case) patch offline time\n",
    "        for j in range(self.num_patches):\n",
    "            self.patch_offline_time_remaining = np.zeros(self.num_patches, float)\n",
    "            self.patch_offline_time_remaining[j] = self.patch_offline_duration\n",
    "            time_overhead = 0.0\n",
    "            \n",
    "            distillation_cycles = self._cycles_per_distillation()\n",
    "            if distillation_cycles is None:\n",
    "                offline_count += 1\n",
    "            else:\n",
    "                time_overheads.append(distillation_cycles / baseline_distillation_cycles)\n",
    "        self._reset()\n",
    "\n",
    "        self.save_cache()\n",
    "\n",
    "        return np.mean(time_overheads), offline_count / self.num_patches\n",
    "\n",
    "    def calculate_avg_overhead_one_phys_qubit_offline(\n",
    "            self,\n",
    "        ): \n",
    "        \"\"\"TODO\n",
    "            \n",
    "        Returns:\n",
    "            Average additional cycle cost when one patch is taken offline.\n",
    "        \"\"\"\n",
    "        # this will be way too expensive unless caching is enabled\n",
    "        assert self._cache_cycles_per_distillation\n",
    "        self._reset()\n",
    "        baseline_distillation_cycles = self._cycles_per_distillation()\n",
    "        assert baseline_distillation_cycles is not None\n",
    "        time_overheads = []\n",
    "        offline_count = 0\n",
    "        # for each ray result, calculate expected (average case) patch offline time\n",
    "        for j in range(self.num_phys_qubits):\n",
    "            self.physical_qubit_offline_time_remaining = np.zeros(self.num_phys_qubits, float)\n",
    "            self.physical_qubit_offline_time_remaining[j] = self.patch_offline_duration\n",
    "            time_overhead = 0.0\n",
    "            \n",
    "            distillation_cycles = self._cycles_per_distillation()\n",
    "            if distillation_cycles is None:\n",
    "                offline_count += 1\n",
    "            else:\n",
    "                time_overheads.append(distillation_cycles / baseline_distillation_cycles)\n",
    "        self._reset()\n",
    "\n",
    "        self.save_cache()\n",
    "\n",
    "        return np.mean(time_overheads), offline_count / self.num_phys_qubits, time_overheads\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset the factory to its initial state.\"\"\"\n",
    "        self.physical_qubit_offline_time_remaining = np.zeros(self.num_phys_qubits, float)\n",
    "\n",
    "    def simulate(\n",
    "            self, \n",
    "            num_distillations: int,\n",
    "            ray_incidence_rate: float,\n",
    "            ray_detector_spec: RayDetectorSpec,\n",
    "            patch_offline_time: float,\n",
    "            use_mpmath: bool = True,\n",
    "            rng_seed: int | None = None,\n",
    "        ):\n",
    "        \"\"\"Simulate the performance of the factory over a number of rounds, with\n",
    "        cosmic rays.\n",
    "        \n",
    "        Args:\n",
    "            num_distillations: Number of distillations to simulate.\n",
    "            ray_incidence_rate: Chance of a ray per qubit per second.\n",
    "            ray_detector_spec: Contains information about ray and detector\n",
    "                behavior.\n",
    "            patch_offline_time: Amount of time (in seconds) a patch is taken\n",
    "                offline when a ray is detected.\n",
    "            use_mpmath: Whether to use mpmath for higher precision.\n",
    "            rng_seed: Seed for the random number generator.\n",
    "            \n",
    "        Returns:\n",
    "            TODO\n",
    "        \"\"\"\n",
    "        self._reset()\n",
    "\n",
    "        rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "        ray_remove_time = 5*ray_detector_spec.ray_halflife\n",
    "\n",
    "        # calculate baseline chances of patches being turned offline (when no\n",
    "        # ray present)\n",
    "        baseline_patch_offline_chances = self._calc_patch_offline_chances(ray_detector_spec, use_mpmath=use_mpmath)\n",
    "\n",
    "        elapsed_time = 0.0\n",
    "        last_distillation_elapsed_time = 0.0\n",
    "        elapsed_time_per_distillation = []\n",
    "        cosmic_ray_history = []\n",
    "        active_cosmic_rays = []\n",
    "        event_history = []\n",
    "        distillations_accepted = []\n",
    "        distillations_remaining = num_distillations\n",
    "        while distillations_remaining > 0:\n",
    "            self.patch_offline_time_remaining[self.patch_offline_time_remaining > 0.0] -= last_distillation_elapsed_time\n",
    "            self.patch_offline_time_remaining[self.patch_offline_time_remaining < 0.0] = 0.0\n",
    "            for p in np.where(np.isclose(self.patch_offline_time_remaining, 0.0))[0]:\n",
    "                event_history.append(('PATCH_ONLINE', p, elapsed_time, distillations_remaining))\n",
    "                self.patch_offline_time_remaining[p] = 0.0\n",
    "            # remove old rays\n",
    "            active_cosmic_rays = [ray for ray in active_cosmic_rays if elapsed_time-ray[0] < ray_remove_time]\n",
    "\n",
    "            # determine whether we can distill\n",
    "            cycles_per_distillation = self._cycles_per_distillation()\n",
    "            wait_time = 0.0\n",
    "            if cycles_per_distillation is None:\n",
    "                # factory is offline\n",
    "                patches_offline = (self.patch_offline_time_remaining > 0.0)\n",
    "                wait_time = self._wait_for_factory_to_come_online()\n",
    "                event_history.append(('WAIT', wait_time, elapsed_time, distillations_remaining))\n",
    "                patches_that_came_online = (self.patch_offline_time_remaining == 0.0) & patches_offline\n",
    "                for p in np.where(patches_that_came_online)[0]:\n",
    "                    event_history.append(('PATCH_ONLINE', p, elapsed_time+wait_time, distillations_remaining))\n",
    "                cycles_per_distillation = self._cycles_per_distillation()\n",
    "                assert cycles_per_distillation is not None\n",
    "            time_per_distillation = cycles_per_distillation * self.cycle_time + wait_time\n",
    "\n",
    "            # generate new cosmic rays\n",
    "            num_cosmic_rays = rng.poisson(ray_incidence_rate * time_per_distillation * self.num_phys_qubits)\n",
    "            new_rays_this_round = []\n",
    "            for i in range(num_cosmic_rays):\n",
    "                center_qubit = rng.choice(np.arange(self.num_phys_qubits))\n",
    "                event_history.append(('RAY', center_qubit, elapsed_time, distillations_remaining))\n",
    "                new_rays_this_round.append((elapsed_time, center_qubit))\n",
    "\n",
    "            elapsed_time += time_per_distillation\n",
    "\n",
    "            patch_no_signal_chances = 1-baseline_patch_offline_chances\n",
    "            # calculate chances of signals due to new rays\n",
    "            for i,ray in enumerate(new_rays_this_round):\n",
    "                patch_no_signal_chances *= np.prod([1-self._calc_patch_offline_chances(ray_detector_spec, ray_incidence_qubit=ray[1], cycles_after_ray_impact=c, use_mpmath=use_mpmath) for c in range(cycles_per_distillation)], axis=0)\n",
    "            # calculate chances of signals due to old rays\n",
    "            for i,ray in enumerate(active_cosmic_rays):\n",
    "                patch_no_signal_chances *= (1-self._calc_patch_offline_chances(ray_detector_spec, ray_incidence_qubit=ray[1], time_after_ray_impact=elapsed_time-ray[0], use_mpmath=use_mpmath))**cycles_per_distillation\n",
    "            patch_signal_chances = 1-patch_no_signal_chances\n",
    "            \n",
    "            # randomly decide if signals are generated; if so, discard\n",
    "            # distillation and turn patches offline. Note: patches that are\n",
    "            # already offline can still be triggered again; this will reset\n",
    "            # their offline time.\n",
    "            patch_signal_decisions = rng.random(self.num_patches) < patch_signal_chances\n",
    "            if np.any(patch_signal_decisions):\n",
    "                for p in np.where(patch_signal_decisions)[0]:\n",
    "                    # record event if patch was not already offline\n",
    "                    if self.patch_offline_time_remaining[p] == 0.0:\n",
    "                        event_history.append(('PATCH_OFFLINE', p, elapsed_time, distillations_remaining))\n",
    "                self.patch_offline_time_remaining[patch_signal_decisions] = patch_offline_time\n",
    "                distillations_accepted.append(False)\n",
    "            else:\n",
    "                distillations_remaining -= 1\n",
    "                distillations_accepted.append(True)\n",
    "            \n",
    "            elapsed_time_per_distillation.append(time_per_distillation)\n",
    "            last_distillation_elapsed_time = time_per_distillation\n",
    "            cosmic_ray_history += new_rays_this_round\n",
    "            active_cosmic_rays += new_rays_this_round\n",
    "        return elapsed_time, elapsed_time_per_distillation, event_history, cosmic_ray_history, distillations_accepted\n",
    "\n",
    "    def simulate_ray_impacts(\n",
    "            self,\n",
    "            ray_detector_spec: RayDetectorSpec,\n",
    "            num_rays: int | None = None,\n",
    "            use_mpmath: bool = False,\n",
    "            rng_seed: int | None = None,\n",
    "            rng: np.random.Generator | None = None,\n",
    "        ):\n",
    "        \"\"\"Simulate the detection of a number of cosmic rays on the factory. Can\n",
    "        be used to calculate the average overhead for the factory.\n",
    "        \n",
    "        Args:\n",
    "            num_rays: Number of ray impacts to simulate. If None, simulate once\n",
    "                for each physical qubit.\n",
    "            ray_detector_spec: Contains information about ray and detector\n",
    "                behavior.\n",
    "            use_mpmath: Whether to use mpmath for higher precision.\n",
    "            rng_seed: Seed for the random number generator.\n",
    "            rng: Random number generator to use.\n",
    "            tail_batch_duration: Discretization of the detection simulation\n",
    "                during the exponential decay of the ray. Smaller values give\n",
    "                more accurate results, but take longer to compute.\n",
    "        \n",
    "         Returns:\n",
    "            TODO\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # center of each ray is a randomly-chosen qubit\n",
    "        # TODO: allow for rays centered just outside of factory\n",
    "        if num_rays is None or num_rays >= self.num_phys_qubits:\n",
    "            impacted_qubits = np.arange(self.num_phys_qubits)\n",
    "        else:\n",
    "            if rng is None:\n",
    "                rng = np.random.default_rng(rng_seed)\n",
    "            impacted_qubits = rng.choice(np.arange(self.num_phys_qubits), num_rays, replace=False)\n",
    "\n",
    "        for q in impacted_qubits:\n",
    "            qubit_detection_chances = self._calc_qubit_offline_chances_in_first_distillation(ray_detector_spec, q, use_mpmath=use_mpmath)\n",
    "            results.append((q, qubit_detection_chances))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _cycles_per_distillation(\n",
    "            self,\n",
    "        ) -> int | None:\n",
    "        \"\"\"Calculate the number of cycles required for one magic state\n",
    "        distillation, based on the current state of\n",
    "        self.patch_offline_time_remaining.\n",
    "\n",
    "        This default function return 6*self.dm if all patches are online, and\n",
    "        None otherwise. Subclasses may override this function to implement more\n",
    "        complex behavior.\n",
    "        \n",
    "        Returns:\n",
    "            The number of surface code stabilizer measurement cycles required\n",
    "            for one magic state distillation, or None if the factory cannot\n",
    "            currently produce magic states.\n",
    "        \"\"\"\n",
    "        physical_qubits_online = (self.physical_qubit_offline_time_remaining <= 0.0)\n",
    "\n",
    "        cycle_count = None\n",
    "        if np.all(physical_qubits_online):\n",
    "            # operating at full capacity\n",
    "            return 6*self.dm\n",
    "        else:\n",
    "            # some patches are offline, and we haven't seen it before\n",
    "            return None\n",
    "\n",
    "    def _wait_for_factory_to_come_online(self):\n",
    "        \"\"\"Wait for the factory to come online, and return the amount of time\n",
    "        waited. Requires that self._cycles_per_distillation() is None. Modifies\n",
    "        self.patch_offline_time_remaining.\n",
    "        \n",
    "        Returns:\n",
    "            Amount of time waited (in seconds). Once this function returns,\n",
    "            self._cycles_per_distillation() will not be None.\n",
    "        \"\"\"\n",
    "        assert self._cycles_per_distillation() is None\n",
    "        wait_time = 0.0\n",
    "        patches_in_order = np.argsort(self.patch_offline_time_remaining)\n",
    "        for delay_time in np.min(self.patch_offline_time_remaining) + np.diff(self.patch_offline_time_remaining[patches_in_order]):\n",
    "            wait_time += delay_time\n",
    "            self.patch_offline_time_remaining -= delay_time\n",
    "            self.patch_offline_time_remaining[self.patch_offline_time_remaining < 0.0] = 0.0\n",
    "            self.patch_offline_time_remaining[np.isclose(self.patch_offline_time_remaining, 0.0)] = 0.0\n",
    "            if self._cycles_per_distillation() is not None:\n",
    "                return wait_time\n",
    "        # should never reach this point\n",
    "        print(self.patch_offline_time_remaining, self._cycles_per_distillation())\n",
    "        raise Exception('Factory never came online.')\n",
    "\n",
    "    def _calc_qubit_offline_chances_in_first_distillation(\n",
    "            self,\n",
    "            ray_detector_spec: RayDetectorSpec,\n",
    "            ray_incidence_qubit: int | None = None,\n",
    "            use_mpmath: bool = False,\n",
    "        ) -> NDArray:\n",
    "        \"\"\"Calculate the chance that each qubit is turned offline due to a ray\n",
    "        event.\n",
    "\n",
    "        Args:\n",
    "            ray_incidence_qubit: Index of the qubit where the ray is incident.\n",
    "            ray_detector_spec: Argument to pass on to RayDetectorSpec.\n",
    "        \n",
    "        Returns:\n",
    "            1D array of length self.num_patches, where each entry is the chance\n",
    "            that the corresponding patch is taken offline due to a ray event.\n",
    "        \"\"\"\n",
    "        if ray_incidence_qubit is None:\n",
    "            physical_qubit_distances_from_ray = np.full(len(self.patch_idx_from_physical_qubit_idx), 1e10)\n",
    "        else:\n",
    "            ray_incidence_coords = self.physical_qubit_coords_from_idx[ray_incidence_qubit]\n",
    "            physical_qubit_distances_from_ray = np.linalg.norm(self.physical_qubit_coords_from_idx - ray_incidence_coords, axis=1)\n",
    "            assert physical_qubit_distances_from_ray.shape == (len(self.patch_idx_from_physical_qubit_idx),)\n",
    "        \n",
    "        qubit_detection_chances = ray_detector_spec.first_distillation_chance(physical_qubit_distances_from_ray)\n",
    "        if use_mpmath:\n",
    "            qubit_detection_chances = np.array([mpmath.mpf(x) for x in qubit_detection_chances], dtype=mpmath.mpf)\n",
    "\n",
    "        return qubit_detection_chances\n",
    "\n",
    "@dataclass\n",
    "class SimulationResult:\n",
    "    \"\"\"Contains the results of a many-round magic state factory simulation.\n",
    "\n",
    "    Attributes:\n",
    "        TODO\n",
    "    \"\"\"\n",
    "    ray_history: list[tuple[int, tuple[int, int]]]\n",
    "\n",
    "def boolean_array_BFS(\n",
    "        array: NDArray, \n",
    "        start: tuple[int, int], \n",
    "        end: tuple[int, int] | None = None,\n",
    "    ) -> dict[tuple[int, int], list[tuple[int, int]]] | list[tuple[int, int]] | None:\n",
    "    \"\"\"Use breadth-first search to find a path through the array from start to\n",
    "    end.\n",
    "\n",
    "    Args:\n",
    "        array: 2D boolean array, where False values are considered obstacles.\n",
    "        start: Starting coordinates.\n",
    "        end: Ending coordinates, or None to find all paths.\n",
    "    \n",
    "    Returns:\n",
    "        If end is None, a dictionary of all paths from start to any True\n",
    "        coordinate pair in the array. If end is not None, a list of coordinates\n",
    "        representing the shortest path from start to end, or None if there is no\n",
    "        path. \n",
    "    \"\"\"\n",
    "    frontier = [start]\n",
    "    paths_to = {start: [start]}\n",
    "    while len(frontier) > 0:\n",
    "        current = frontier[0]\n",
    "        frontier = frontier[1:]\n",
    "        neighbors = [\n",
    "            (current[0]+1, current[1]), \n",
    "            (current[0]-1, current[1]), \n",
    "            (current[0], current[1]+1), \n",
    "            (current[0], current[1]-1)\n",
    "        ]\n",
    "        for n in neighbors:\n",
    "            if n[0] >= 0 and n[0] < array.shape[0] and n[1] >= 0 and n[1] < array.shape[1] and array[n]:\n",
    "                if n not in paths_to:\n",
    "                    # first time visiting n\n",
    "                    paths_to[n] = paths_to[current] + [n]\n",
    "                    frontier.append(n)\n",
    "                    if n == end:\n",
    "                        break\n",
    "                else:\n",
    "                    # already visited n\n",
    "                    if len(paths_to[current] + [n]) < len(paths_to[n]):\n",
    "                        paths_to[n] = paths_to[current] + [n]\n",
    "\n",
    "    if end is None:\n",
    "        return paths_to\n",
    "    else:\n",
    "        if end not in paths_to:\n",
    "            return None\n",
    "        else:\n",
    "            return paths_to[end]\n",
    "\n",
    "def boolean_array_all_pairs_BFS(\n",
    "        array: NDArray,\n",
    "    ) -> dict[tuple[int, int], dict[tuple[int, int], list[tuple[int, int]]]]:\n",
    "    \"\"\"Compute all pairs shortest paths.\n",
    "    \n",
    "    Args:\n",
    "        array: 2D boolean array, where False values are considered obstacles.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of dictionaries, where the outer dictionary maps starting\n",
    "        coordinates to inner dictionaries, which map ending coordinates to paths\n",
    "        from starting coordinates to ending coordinates.\n",
    "    \"\"\"\n",
    "    all_paths = {}\n",
    "    for i in range(array.shape[0]):\n",
    "        for j in range(array.shape[1]):\n",
    "            if array[i,j]:\n",
    "                all_paths[(i,j)] = boolean_array_BFS(array, (i,j))\n",
    "    return all_paths\n",
    "\n",
    "class Redundant15To1(MagicStateFactory):\n",
    "    \"\"\"TODO\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            dx: int = 7,\n",
    "            dz: int = 7,\n",
    "            dm: int = 7,\n",
    "            patch_offline_duration: float = 30e-3,\n",
    "            cycle_time: float | None = None,\n",
    "            cache_cycles_per_distillation: bool = False,\n",
    "            num_redundant_cols: int = 0,\n",
    "            redundant_top_routing_space: int = 0,\n",
    "            redundant_bot_routing_space: int = 0,\n",
    "            mapping_mode: str = 'simple',\n",
    "        ):\n",
    "        \"\"\"Initializes the factory, using a layout from litinski_magic_2019.\n",
    "\n",
    "        Args:\n",
    "            dx: X code distance for each patch.\n",
    "            dz: Z code distance for each patch.\n",
    "            dm: Temporal code distance.\n",
    "            patch_offline_duration: Amount of time (in seconds) a patch is taken\n",
    "                offline when a ray is detected.\n",
    "            cycle_time: Duration of a single surface code stabilizer measurement\n",
    "                cycle. If None, will be calculated based on the code distance.\n",
    "            cache_cycles_per_distillation: If True, store results of previous\n",
    "                calls to _cycles_per_distillation() to avoid redundant\n",
    "                calculations.\n",
    "            num_redundant_cols: Number of redundant patches to include.\n",
    "            redundant_top_routing_space: Number of redundant rows to include\n",
    "                above the top routing space.\n",
    "            redundant_bot_routing_space: Number of redundant rows to include\n",
    "                below the bottom routing space.\n",
    "            mapping_mode: 'simple', 'rearrange_greedy', or 'rearrange_full'. Method\n",
    "                'simple' does not rearrange logical central patches, and simply\n",
    "                checks to see if the patches can still communicate with each\n",
    "                other. Method 'rearrange_greedy' rearranges logical central\n",
    "                patches into a new configuration, but does not fully optimize\n",
    "                the new configuration for cycle cost. Method 'rearrange_full'\n",
    "                fully optimizes the new configuration for cycle cost.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            dx, \n",
    "            dz, \n",
    "            dm, \n",
    "            patch_offline_duration, \n",
    "            cycle_time, \n",
    "            cache_cycles_per_distillation,\n",
    "            f'data/mapping_cache_{mapping_mode}.pkl',\n",
    "        )\n",
    "\n",
    "        self.patch_offline_duration = patch_offline_duration\n",
    "\n",
    "        self.logical_qubit_row = 1 + redundant_top_routing_space\n",
    "\n",
    "        num_rows = 3 + redundant_top_routing_space + redundant_bot_routing_space\n",
    "        num_cols = 5 + num_redundant_cols\n",
    "        self.current_wide_columns = [False]*num_cols\n",
    "        self.current_wide_columns[0] = True\n",
    "        row_heights = np.full(num_rows, self.dx)\n",
    "        col_widths = np.array([self.dx]*1 + [self.dz]*(4+num_redundant_cols))\n",
    "\n",
    "        self.num_patches = num_rows * num_cols\n",
    "        self.patch_indices = np.reshape(np.arange(self.num_patches), (num_rows, num_cols))\n",
    "        self.patch_coords_from_idx = np.zeros((self.num_patches, 2), int)\n",
    "        for idx in range(self.num_patches):\n",
    "            self.patch_coords_from_idx[idx] = [idx // self.patch_indices.shape[1], idx % self.patch_indices.shape[1]]\n",
    "\n",
    "        # use stim_surface_code.patch to generate the physical qubit array\n",
    "        # TODO: currently does not account for extra 4*dm space needed for magic\n",
    "        # state injection (see figs. 10-11 of Litinski). Is this important?\n",
    "        surface_code_patch = patch.SurfaceCodePatch(sum(row_heights) + (num_rows-1), sum(col_widths) + (num_cols-1), dm)\n",
    "        self.physical_qubit_array = np.array([[(q.idx if q is not None else -1) for q in row] for row in surface_code_patch.device], int)\n",
    "        self.physical_qubit_coords_from_idx = np.zeros((len(surface_code_patch.all_qubits), 2), int)\n",
    "        for q in surface_code_patch.all_qubits:\n",
    "            self.physical_qubit_coords_from_idx[q.idx] = q.coords\n",
    "        if cycle_time is None:\n",
    "            self.cycle_time = surface_code_patch.cycle_time()\n",
    "        else:\n",
    "            self.cycle_time = cycle_time\n",
    "\n",
    "        self.num_phys_qubits = len(surface_code_patch.all_qubits)\n",
    "        self.physical_qubit_offline_time_remaining = np.zeros(self.num_phys_qubits, float)\n",
    "\n",
    "        self.prev_optimization_key = None\n",
    "        self.prev_distillation_cycles = 6\n",
    "\n",
    "        # see Litinski Fig. 3\n",
    "        self.applied_rotations = np.array([\n",
    "            [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0],\n",
    "            [1,1,0,0,1,0,1,0,1,0,0,0,1,0,1,1],\n",
    "            [1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,1],\n",
    "            [0,1,1,1,0,0,1,0,0,0,1,0,1,1,1,0],\n",
    "            [0,0,0,1,1,1,1,0,0,0,0,1,0,1,1,1]\n",
    "        ])\n",
    "        # These rotations must be applied before the T state is moved out.\n",
    "        self.initial_rotations = self.applied_rotations[:, :6]\n",
    "        # These rotations can be applied after the T state is moved out.\n",
    "        self.final_rotations = self.applied_rotations[:, 6:]\n",
    "\n",
    "        self.mapping_mode = mapping_mode\n",
    "\n",
    "    def _get_phys_qubit_patch_mapping(\n",
    "            self,\n",
    "            wide_columns: list[bool],\n",
    "        ):\n",
    "        \"\"\"TODO\n",
    "        \"\"\"\n",
    "        col_widths = np.array([self.dx if wide_columns[i] else self.dz for i in range(self.patch_indices.shape[1])])\n",
    "        patch_idx_from_physical_qubit_idx = np.full(self.num_phys_qubits, -1, dtype=int)\n",
    "        physical_qubits_from_patch_idx = [[] for _ in range(self.num_patches)]\n",
    "        for row in range(self.patch_indices.shape[0]):\n",
    "            min_phys_row = np.sum(2*self.dx*row+1) + row\n",
    "            max_phys_row = min_phys_row + 2*self.dx+1\n",
    "            for col in range(self.patch_indices.shape[1]):\n",
    "                min_phys_col = np.sum(2*col_widths[:col]+1) + col\n",
    "                max_phys_col = min_phys_col + 2*col_widths[col]+1\n",
    "                for phys_row in range(min_phys_row, max_phys_row):\n",
    "                    for phys_col in range(min_phys_col, max_phys_col):\n",
    "                        phys_idx = self.physical_qubit_array[phys_row, phys_col]\n",
    "                        if phys_idx != -1:\n",
    "                            patch_idx_from_physical_qubit_idx[phys_idx] = self.patch_indices[row, col]\n",
    "                            physical_qubits_from_patch_idx[self.patch_indices[row, col]].append(phys_idx)\n",
    "        return patch_idx_from_physical_qubit_idx, physical_qubits_from_patch_idx\n",
    "    \n",
    "    def _get_patches_online(\n",
    "            self,\n",
    "            wide_columns: list[bool] | None = None,\n",
    "        ):\n",
    "        if wide_columns is None:\n",
    "            wide_columns = self.current_wide_columns\n",
    "        patch_idx_from_physical_qubit_idx, physical_qubits_from_patch_idx = self._get_phys_qubit_patch_mapping(wide_columns)\n",
    "        patches_online = np.zeros(self.num_patches, bool)\n",
    "        for p in range(self.num_patches):\n",
    "            patches_online[p] = np.all(self.physical_qubit_offline_time_remaining[physical_qubits_from_patch_idx[p]] <= 0.0)\n",
    "        return patches_online\n",
    "\n",
    "    def _cycles_per_distillation(\n",
    "            self,\n",
    "        ) -> int | None:\n",
    "        \"\"\"Calculate the number of cycles required for one magic state\n",
    "        distillation, based on the current state of\n",
    "        self.patch_offline_time_remaining.\n",
    "\n",
    "        Even if self.cache_cycles_per_distillation is False, this function\n",
    "        remembers the result of the last call to this function, and will return\n",
    "        the same result if the state of self.patch_offline_time_remaining has\n",
    "        not changed. This uses barely any memory, and is useful for avoiding\n",
    "        redundant calculations.\n",
    "        \n",
    "        Returns:\n",
    "            The number of surface code stabilizer measurement cycles required\n",
    "            for one magic state distillation, or None if the factory cannot\n",
    "            currently produce magic states.\n",
    "        \"\"\"\n",
    "        physical_qubits_online = (self.physical_qubit_offline_time_remaining <= 0.0)\n",
    "        \n",
    "        cycle_count = None\n",
    "        if np.all(physical_qubits_online):\n",
    "            # operating at full capacity\n",
    "            cycle_count = 6\n",
    "        else:\n",
    "            if self.mapping_mode == 'none':\n",
    "                cycle_count = None\n",
    "            elif self.mapping_mode == 'simple':\n",
    "                cycle_count = self._cycles_per_distillation_simple(\n",
    "                    physical_qubits_online,\n",
    "                )\n",
    "            elif self.mapping_mode == 'remap':\n",
    "                cycle_count = self._cycles_per_distillation_remap(\n",
    "                    physical_qubits_online, \n",
    "                )[0]\n",
    "            else:\n",
    "                raise ValueError('Invalid mode.')\n",
    "        return cycle_count\n",
    "\n",
    "    def _cycles_per_distillation_simple(\n",
    "            self, \n",
    "            physical_qubits_online: NDArray[np.bool_],\n",
    "        ) -> int | None:\n",
    "        patch_idx_from_physical_qubit_idx, physical_qubits_from_patch_idx = self._get_phys_qubit_patch_mapping([True]+[False]*(self.patch_indices.shape[1]-1))\n",
    "        patches_online = np.zeros(self.num_patches, bool)\n",
    "        for p in range(self.num_patches):\n",
    "            patches_online[p] = np.all(physical_qubits_online[physical_qubits_from_patch_idx[p]])\n",
    "\n",
    "        current_optimization_key = (tuple(self.current_wide_columns), tuple(patches_online))\n",
    "        if current_optimization_key in self._cycles_per_distillation_cache:\n",
    "            return self._cycles_per_distillation_cache[(tuple(self.current_wide_columns), tuple(patches_online))]\n",
    "\n",
    "        array = patches_online.astype(float)\n",
    "        array[array == 0] = -np.inf\n",
    "        array = array[self.patch_indices]\n",
    "        array[:,0] *= 100\n",
    "\n",
    "        one_routing_space = np.ones((2,5))\n",
    "        two_routing_spaces = np.ones((3,5))\n",
    "\n",
    "        results_one_routing = scipy.signal.convolve(array, one_routing_space, mode='valid')\n",
    "        results_two_routing = scipy.signal.convolve(array, two_routing_spaces, mode='valid')\n",
    "\n",
    "        if np.max(results_two_routing) > 100:\n",
    "            cycle_count = 6\n",
    "        elif np.max(results_one_routing) > 100:\n",
    "            cycle_count = 12\n",
    "        else:\n",
    "            cycle_count = None\n",
    "\n",
    "        if self._cache_cycles_per_distillation:\n",
    "            self._cycles_per_distillation_cache[current_optimization_key] = cycle_count\n",
    "\n",
    "        return cycle_count\n",
    "\n",
    "    def _cycles_per_distillation_remap(\n",
    "            self,\n",
    "            physical_qubits_online: NDArray[np.bool_],\n",
    "        ) -> tuple[int | None, NDArray[np.int_] | None]:\n",
    "        \n",
    "        best_cycle_count = None\n",
    "        best_mapping = None\n",
    "\n",
    "        for chosen_wide_column in range(self.patch_indices.shape[1]):\n",
    "            wide_columns = [False]*self.patch_indices.shape[1]\n",
    "            wide_columns[chosen_wide_column] = True\n",
    "            _, physical_qubits_from_patch_idx = self._get_phys_qubit_patch_mapping([True]+[False]*(self.patch_indices.shape[1]-1))\n",
    "\n",
    "            patches_online = np.zeros(self.num_patches, bool)\n",
    "            for p in range(self.num_patches):\n",
    "                patches_online[p] = np.all(physical_qubits_online[physical_qubits_from_patch_idx[p]])\n",
    "            online_patch_indices = np.where(patches_online)[0]\n",
    "            online_wide_patches = [i for i in self.patch_indices[:,chosen_wide_column] if patches_online[i]]\n",
    "\n",
    "            current_optimization_key = (tuple(wide_columns), tuple(patches_online))\n",
    "            if current_optimization_key in self._cycles_per_distillation_cache:\n",
    "                cycle_count = self._cycles_per_distillation_cache[(tuple(wide_columns), tuple(patches_online))]\n",
    "                if cycle_count is not None and (best_cycle_count is None or cycle_count < best_cycle_count):\n",
    "                    best_cycle_count = cycle_count\n",
    "                    best_mapping = None\n",
    "            else:\n",
    "                for chosen_q0 in online_wide_patches:\n",
    "                    for chosen_other_qubits in itertools.combinations([i for i in online_patch_indices if i != chosen_q0], 4):\n",
    "                        chosen_logical_qubits = np.array([chosen_q0] + list(chosen_other_qubits), int)\n",
    "                        communication_paths = [[] for _ in range(self.applied_rotations.shape[1])]\n",
    "\n",
    "                        # make sure we can perform each rotation\n",
    "                        can_perform_rotations = np.zeros(self.applied_rotations.shape[1], bool)\n",
    "                        search_array = patches_online[self.patch_indices]\n",
    "                        for q in chosen_logical_qubits:\n",
    "                            search_array[self.patch_coords_from_idx[q][0], self.patch_coords_from_idx[q][1]] = False\n",
    "                        all_pairs_paths = boolean_array_all_pairs_BFS(search_array)\n",
    "                        for i,combo in enumerate(self.applied_rotations.T):\n",
    "                            qubits = np.where(combo)[0]\n",
    "                            qubit_coords = self.patch_coords_from_idx[chosen_logical_qubits[qubits]]\n",
    "                            # routing space must connect to either top or bottom of\n",
    "                            # each; we will check all combinations until we find one\n",
    "                            # that works\n",
    "                            # TODO: if a qubit is in a wide column, it can\n",
    "                            # optionally be rotated and use left/right instead of\n",
    "                            # top/bottom\n",
    "                            for top_bot_assignment in itertools.product([-1, 1], repeat=len(qubits)):\n",
    "                                routing_coords = qubit_coords + np.array([list(top_bot_assignment), [0]*len(qubits)]).T\n",
    "                                if np.any(routing_coords < 0) or np.any(routing_coords[:,0] >= self.patch_indices.shape[0]) or np.any(routing_coords[:,1] >= self.patch_indices.shape[1]):\n",
    "                                    continue\n",
    "                                communication_path = set()\n",
    "                                can_perform_rotations[i] = True\n",
    "                                for q0 in range(len(qubits)):\n",
    "                                    if tuple(routing_coords[q0]) in all_pairs_paths:\n",
    "                                        paths = all_pairs_paths[tuple(routing_coords[q0])]\n",
    "                                        for q1 in range(q0, len(qubits)):\n",
    "                                            if tuple(routing_coords[q1]) in paths:\n",
    "                                                shortest_path = np.array(paths[tuple(routing_coords[q1])])\n",
    "                                                communication_path |= set(self.patch_indices[shortest_path[:,0], shortest_path[:,1]])\n",
    "                                            else:\n",
    "                                                can_perform_rotations[i] = False\n",
    "                                                break\n",
    "                                    else:\n",
    "                                        can_perform_rotations[i] = False\n",
    "                                        break\n",
    "                                    if not can_perform_rotations[i]:\n",
    "                                        break\n",
    "                                if can_perform_rotations[i]:\n",
    "                                    assert len(communication_path) > 0, (i, all_pairs_paths[tuple(routing_coords[0])], communication_path)\n",
    "                                    communication_paths[i].append(communication_path)\n",
    "                        \n",
    "                        if not np.all(can_perform_rotations):\n",
    "                            continue\n",
    "                        else:\n",
    "                            num_layers = 0\n",
    "                            unscheduled_rotations = list(range(self.initial_rotations.shape[1]))\n",
    "                            rotation_order = []\n",
    "                            rotation_layers = []\n",
    "                            did_initial_rotations = False\n",
    "                            did_final_rotations = False\n",
    "\n",
    "                            while not did_final_rotations:\n",
    "                                if len(unscheduled_rotations) == 0:\n",
    "                                    if did_initial_rotations:\n",
    "                                        did_final_rotations = True\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        did_initial_rotations = True\n",
    "                                        unscheduled_rotations = list(range(self.initial_rotations.shape[1], self.applied_rotations.shape[1]))\n",
    "                                # schedule a layer of rotations\n",
    "                                num_layers += 1\n",
    "                                completed_rotations = []\n",
    "                                # We want to minimize number of layers, so we will\n",
    "                                # check every combination of rotations, attempting\n",
    "                                # larger numbers of parallel rotations first.\n",
    "                                for rot_combo in itertools.chain.from_iterable([itertools.combinations(unscheduled_rotations, n) for n in range(5,0,-1)]):\n",
    "                                    all_rots_can_be_done = False\n",
    "                                    for path_combo in itertools.product(*[communication_paths[rot] for rot in rot_combo]):\n",
    "                                        # all paths must not intersect each other\n",
    "                                        paths_sum = sum([len(path) for path in path_combo])\n",
    "                                        paths_set = set.union(*path_combo)\n",
    "                                        if paths_sum == len(paths_set):\n",
    "                                            all_rots_can_be_done = True\n",
    "                                            break\n",
    "                                    if all_rots_can_be_done:\n",
    "                                        completed_rotations = list(rot_combo)\n",
    "                                        for rot in completed_rotations:\n",
    "                                            rotation_order.append(self.applied_rotations.T[rot])\n",
    "                                        rotation_layers.extend([num_layers]*len(completed_rotations))\n",
    "                                        break\n",
    "                                assert len(completed_rotations) > 0\n",
    "                                unscheduled_rotations = [x for x in unscheduled_rotations if x not in completed_rotations]\n",
    "                            if best_cycle_count is None or num_layers < best_cycle_count:\n",
    "                                best_cycle_count = num_layers\n",
    "                                best_mapping = chosen_logical_qubits\n",
    "                        if best_cycle_count < 6:\n",
    "                            raise Exception('Cycle time is less than 6.')\n",
    "                        if best_cycle_count == 6:\n",
    "                            # we know we can't do better than this\n",
    "                            if self._cache_cycles_per_distillation:\n",
    "                                self._cycles_per_distillation_cache[current_optimization_key] = best_cycle_count\n",
    "                            return best_cycle_count, best_mapping\n",
    "                if self._cache_cycles_per_distillation:\n",
    "                    self._cycles_per_distillation_cache[current_optimization_key] = best_cycle_count\n",
    "        return best_cycle_count, best_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "factory = Redundant15To1(dx=7, dz=3, dm=3, mapping_mode='remap', cache_cycles_per_distillation=True)\n",
    "# patches_online = np.array([True]*factory.num_patches)\n",
    "# patches_online = patches_online[factory.patch_indices]\n",
    "# patches_online[1,0] = False\n",
    "# patches_online = patches_online.flatten()\n",
    "# factory.patch_offline_time_remaining = ~patches_online\n",
    "factory.physical_qubit_offline_time_remaining = np.zeros(factory.num_phys_qubits, float)\n",
    "factory.physical_qubit_offline_time_remaining[0] = 1\n",
    "# factory.physical_qubit_offline_time_remaining[10] = 1\n",
    "print(factory._cycles_per_distillation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory._cycles_per_distillation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = Redundant15To1(dx=7, dz=3, dm=3, mapping_mode='remap', cache_cycles_per_distillation=True)\n",
    "# factory = Redundant15To1(dx=7, dz=7, dm=7, num_redundant_cols=1, redundant_top_routing_space=1, redundant_bot_routing_space=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19574074074074074"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chance of a ray hitting within a 50ms window\n",
    "factory.num_phys_qubits * 1/27/10 * 50e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8222253765895644,\n",
       " 0.16094300426947586,\n",
       " 0.015751551436373704,\n",
       " 0.0010277401153238892,\n",
       " 5.029265286561811e-05]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[scipy.stats.poisson.pmf(k, factory.num_phys_qubits * 1/27/10 * 50e-3) for k in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_detector_spec = dill.load(open('data/ray_detector_spec_fpr_1e-13.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_overhead(ray_rate, recovery_time, online_chance, online_overhead):\n",
    "    prob_ray = ray_rate*recovery_time\n",
    "\n",
    "    # distillations per time, relative to no rays\n",
    "    ts_per_time_relative = (1-prob_ray) + prob_ray*online_chance/online_overhead\n",
    "    return 1/ts_per_time_relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, 0.0) 1.2433801519686853\n",
      "(2.037030871240402, 0.13007038616885808) 1.2891520647728552\n",
      "(1.9772698586467705, 0.12979542157266338) 1.2807194570524154\n",
      "(1.912290711463337, 0.16676239176737656) 1.2835359761062406\n",
      "(1.9706585356235757, 0.20217625422368343) 1.303763422168252\n"
     ]
    }
   ],
   "source": [
    "for n in [1, 10, 20, 30, 50]:\n",
    "    result = factory.calculate_avg_overhead_per_ray(ray_detector_spec, n, 1000, 1e-8)\n",
    "    print(result, expected_overhead(1/27/10*factory.num_phys_qubits, 100e-3, 1-result[1], result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "print(len(factory._cycles_per_distillation_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0033109260559847"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_overhead(0.2, 30e-3, 0.9, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple\n",
      "Standard factory: \t1.85x average time overhead, 0.30 inoperational rate\n",
      "Additional column: \t1.69x average time overhead, 0.26 inoperational rate, 1.17x qubit overhead\n"
     ]
    }
   ],
   "source": [
    "print('Simple')\n",
    "factory = Redundant15To1(dx=7, dz=3, dm=3, mapping_mode='simple', cache_cycles_per_distillation=True)\n",
    "time_overhead_simple_0 = factory.calculate_avg_overhead_one_phys_qubit_offline()\n",
    "print(f'Standard factory: \\t{time_overhead_simple_0[0]:0.2f}x average time overhead, {time_overhead_simple_0[1]:0.2f} inoperational rate')\n",
    "baseline_phys_qubits = factory.num_phys_qubits\n",
    "\n",
    "factory = Redundant15To1(dx=7, dz=3, dm=3, num_redundant_cols=1, mapping_mode='simple', cache_cycles_per_distillation=True)\n",
    "time_overhead_simple_1 = factory.calculate_avg_overhead_one_phys_qubit_offline()\n",
    "print(f'Additional column: \\t{time_overhead_simple_1[0]:0.2f}x average time overhead, {time_overhead_simple_1[1]:0.2f} inoperational rate, {factory.num_phys_qubits / baseline_phys_qubits:0.2f}x qubit overhead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapping\n",
      "Standard factory: \t1.76x average time overhead, 0.00 inoperational rate\n"
     ]
    }
   ],
   "source": [
    "print('Remapping')\n",
    "factory = Redundant15To1(dx=7, dz=3, dm=3, mapping_mode='remap', cache_cycles_per_distillation=True)\n",
    "time_overhead_remap_0 = factory.calculate_avg_overhead_one_phys_qubit_offline()\n",
    "print(f'Standard factory: \\t{time_overhead_remap_0[0]:0.2f}x average time overhead, {time_overhead_remap_0[1]:0.2f} inoperational rate')\n",
    "baseline_phys_qubits = factory.num_phys_qubits\n",
    "\n",
    "factory = Redundant15To1(dx=7, dz=3, dm=3, num_redundant_cols=1, mapping_mode='remap', cache_cycles_per_distillation=True)\n",
    "time_overhead_remap_1 = factory.calculate_avg_overhead_one_phys_qubit_offline()\n",
    "print(f'Additional column: \\t{time_overhead_remap_1[0]:0.2f}x average time overhead, {time_overhead_remap_1[1]:0.2f} inoperational rate, {factory.num_phys_qubits / baseline_phys_qubits:0.2f}x qubit overhead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-delay-mG6OZh6y-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
