{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import scipy\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import itertools\n",
    "import mpmath\n",
    "import dill\n",
    "import pickle\n",
    "import os\n",
    "import multiprocess as mp\n",
    "\n",
    "from ray_delay.ray_detector import RayDetectorSpec, RayImpactSimulator\n",
    "from stim_surface_code import patch, memory\n",
    "from ray_delay.factory_simulator import Redundant15To1\n",
    "from ray_delay.noise_model_patch import NoiseModelPatch\n",
    "from ray_delay.noise_model import RayModelType, CosmicRayParams\n",
    "\n",
    "import qc_utils.stats\n",
    "import qc_utils.plot\n",
    "\n",
    "mpmath.mp.dps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 7\n",
    "dz = 3\n",
    "dm = 3\n",
    "\n",
    "f = Redundant15To1(dx, dz, dm, cache_cycles_per_distillation=True, mapping_mode='remap')\n",
    "buffer_patch = memory.MemoryPatch(dx, dx, dm)\n",
    "buffer_patch_nqubits = len(buffer_patch.all_qubits)\n",
    "factory = Redundant15To1(7, 3, 3)\n",
    "factory_nqubits = factory.num_phys_qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_qubitcycle_overhead(ray_rate, recovery_time, online_chance, online_overhead):\n",
    "    \"\"\"Calculate the expected qubitcycle overhead for a given ray rate, recovery\n",
    "    time, online chance, and online overhead. Assumes that more than one ray\n",
    "    impact will always result in the factory being offline.\n",
    "    \n",
    "    Args:\n",
    "        ray_rate: The rate of rays in rays/qubit/s.\n",
    "        recovery_time: The time it takes to recover from a ray in seconds.\n",
    "        online_chance: The chance that the factory is online given a ray impact.\n",
    "        online_overhead: The expected qubitcycle overhead, given that a factory\n",
    "            is online, during a ray impact.\n",
    "    \n",
    "    Returns:\n",
    "        The expected qubitcycle overhead.\n",
    "    \"\"\"\n",
    "    prob_no_rays = scipy.stats.poisson.pmf(0, factory_nqubits*ray_rate*recovery_time)\n",
    "    prob_1_ray = scipy.stats.poisson.pmf(1, factory_nqubits*ray_rate*recovery_time)\n",
    "\n",
    "    # distillations per time, relative to no rays\n",
    "    ts_per_time_relative = prob_no_rays + prob_1_ray*online_chance/online_overhead\n",
    "    \n",
    "    return 1/ts_per_time_relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ideal_detector_spec(radius, strength):\n",
    "    return RayDetectorSpec(\n",
    "        detector_spatial_window_size=None,\n",
    "        detector_temporal_window_size=None,\n",
    "        ray_params=CosmicRayParams(min_radius=radius, max_radius=radius, max_strength=strength),\n",
    "        ideal_detection=True,\n",
    "    )\n",
    "\n",
    "def calc_overhead_ideal_detection(factory, radius, strength):\n",
    "    spec = get_ideal_detector_spec(radius, strength)\n",
    "    return factory.calculate_avg_overhead_per_ray(spec, 1000, 1, 1e-7, save_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpmath_poisson_ppf(p, lam, max_k=10):\n",
    "    total_prob = 0\n",
    "    k = 0\n",
    "    while total_prob < p:\n",
    "        total_prob += mpmath.exp(-lam)*mpmath.power(lam, mpmath.mpf(k))/mpmath.factorial(mpmath.mpf(k))\n",
    "        k += 1\n",
    "        if k > max_k:\n",
    "            return 10**10\n",
    "    return k-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_overhead_ideal_detection(ray_rate, recovery_time):\n",
    "    logical_err_rate = mpmath.mpf(1e-10)\n",
    "    max_num_overlapping_rays = 1+mpmath_poisson_ppf(1-logical_err_rate, buffer_patch_nqubits*ray_rate*recovery_time)\n",
    "\n",
    "    code_overheads = {0: 1, 1: 1, 2: 4, 3: 7, 4: 11, 5: 17}\n",
    "    if max_num_overlapping_rays in code_overheads:\n",
    "        return code_overheads[max_num_overlapping_rays]\n",
    "    else:\n",
    "        return 10**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expansion_overhead_ideal_detection(ray_rate, recovery_time, radius, strength, model='direct'):\n",
    "    logical_err_rate = mpmath.mpf(1e-10)\n",
    "    max_num_overlapping_rays = 1+mpmath_poisson_ppf(1-logical_err_rate, buffer_patch_nqubits*ray_rate*recovery_time)\n",
    "\n",
    "    # chance_2_overlapping_rays = scipy.stats.poisson.pmf(2, buffer_patch_nqubits*ray_rate*recovery_time)\n",
    "\n",
    "    if model == 'direct':\n",
    "        extra_d = int(2*radius)\n",
    "    else:\n",
    "        assert model == 'scrambling'\n",
    "        extra_d = int(radius)\n",
    "    f_extra = Redundant15To1(dx+extra_d*max_num_overlapping_rays, dz+extra_d*max_num_overlapping_rays, dm)\n",
    "    return f_extra.num_phys_qubits / factory_nqubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n"
     ]
    }
   ],
   "source": [
    "if generate_data:\n",
    "    radii = np.linspace(2, 8, 10)\n",
    "    strengths = 1-np.geomspace(0.001, 0.1, 12)[::-1]\n",
    "    rates = np.geomspace(1e-7, 3e-3, 100)\n",
    "\n",
    "    factory = Redundant15To1(7, 3, 3, cache_cycles_per_distillation=True, mapping_mode='remap', rng=0)\n",
    "    assert len(factory._cycles_per_distillation_cache) > 0 # make sure cache loaded\n",
    "    with mp.Pool(6) as pool:\n",
    "        results = pool.starmap(calc_overhead_ideal_detection, [(factory, r, 1.0) for r in radii])\n",
    "    remap_time_overheads = np.array([r[0] for r in results])\n",
    "    remap_offline_rates = np.array([r[1] for r in results])\n",
    "\n",
    "    net_overheads_offline = np.zeros((radii.shape[0], rates.shape[0]))\n",
    "    net_overheads_remap = np.zeros((radii.shape[0], rates.shape[0]))\n",
    "    for i,r in enumerate(radii):\n",
    "            for j,rate in enumerate(rates):\n",
    "                    net_overheads_remap[i,j] = expected_qubitcycle_overhead(rate, 1, 1-remap_offline_rates[i], remap_time_overheads[i])\n",
    "                    net_overheads_offline[i,j] = expected_qubitcycle_overhead(rate, 1, 0, 1)\n",
    "\n",
    "    net_overheads_distributed = np.zeros((radii.shape[0], rates.shape[0]))\n",
    "    for j,rate in enumerate(rates):\n",
    "            net_overheads_distributed[:,j] = distributed_overhead_ideal_detection(rate, 1)\n",
    "\n",
    "    strengths = np.array([0.9, 0.99, 0.999])\n",
    "    net_overheads_expansion_direct = np.zeros((radii.shape[0], rates.shape[0], strengths.shape[0]))\n",
    "    net_overheads_expansion_scrambling = np.zeros((radii.shape[0], rates.shape[0]))\n",
    "\n",
    "    for i,r in enumerate(radii):\n",
    "        print(i, f'/ {radii.shape[0]}')\n",
    "        with mp.Pool(6) as pool:\n",
    "            for j,s in enumerate(strengths):\n",
    "                net_overheads_expansion_direct[i,:,j] = pool.starmap(expansion_overhead_ideal_detection, [(rate, 1, r, s, 'direct') for rate in rates])\n",
    "            net_overheads_expansion_scrambling[i,:] = pool.starmap(expansion_overhead_ideal_detection, [(rate, 1, r, 1, 'scrambling') for rate in rates])\n",
    "\n",
    "    with open('data/evaluation_ideal_detection.pkl', 'wb') as f:\n",
    "        dill.dump({\n",
    "            'radii': radii,\n",
    "            'rates': rates,\n",
    "            'strengths': strengths,\n",
    "            'overheads_remap': net_overheads_remap,\n",
    "            'overheads_offline': net_overheads_offline,\n",
    "            'overheads_distributed': net_overheads_distributed,\n",
    "            'overheads_expansion_direct': net_overheads_expansion_direct,\n",
    "            'overheads_expansion_scrambling': net_overheads_expansion_scrambling,\n",
    "        }, f)\n",
    "else:\n",
    "    with open('data/evaluation_ideal_detection.pkl', 'rb') as f:\n",
    "        data = dill.load(f)\n",
    "        radii = data['radii']\n",
    "        rates = data['rates']\n",
    "        strengths = data['strengths']\n",
    "        net_overheads_remap = data['overheads_remap']\n",
    "        net_overheads_offline = data['overheads_offline']\n",
    "        net_overheads_distributed = data['overheads_distributed']\n",
    "        net_overheads_expansion_direct = data['overheads_expansion_direct']\n",
    "        net_overheads_expansion_scrambling = data['overheads_expansion_scrambling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      3\u001b[0m x, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(radii\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, rates, indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m norm \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mcolors\u001b[38;5;241m.\u001b[39mNormalize(vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(24, 8))\n",
    "\n",
    "x, y = np.meshgrid(radii/2, rates, indexing='ij')\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=1, vmax=32)\n",
    "\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'mycmap', \n",
    "    [\n",
    "        (0, '#FFFFFF'),\n",
    "        (norm(2), '#EEDD00'),\n",
    "        (norm(6), '#DD9900'),\n",
    "        (1, '#770000'),\n",
    "    ]                                                    \n",
    ")\n",
    "cmap.set_over('#330000')\n",
    "cmap.set_under('white')\n",
    "cmap.set_bad('#330000')\n",
    "\n",
    "ax[0].pcolormesh(x, y, net_overheads_distributed, norm=norm, cmap=cmap)\n",
    "ax[0].set_title(r'$\\it{Distributed}$ baseline')\n",
    "ax[0].set_ylabel(r'$\\Gamma \\times T_{\\text{offline}}$')\n",
    "\n",
    "ax[1].pcolormesh(x, y, net_overheads_expansion_direct[:,:,1], norm=norm, cmap=cmap)\n",
    "ax[1].set_title(r'$\\it{Code}$ $\\it{expansion}$ baseline' + '\\n' + r'($\\bf{Direct}$ noise model, $f_{T_1} = 0.01$)')\n",
    "\n",
    "with plt.rc_context({'axes.edgecolor':'white', 'xtick.color':'white', 'ytick.color':'white', 'figure.facecolor':'black'}):\n",
    "    ax_inset_1 = ax[1].inset_axes([0.2,0.6,0.3,0.3])\n",
    "    ax_inset_1.pcolormesh(x, y, net_overheads_expansion_direct[:,:,0], norm=norm, cmap=cmap)\n",
    "    ax_inset_1.loglog()\n",
    "    ax_inset_1.tick_params(top=False, left=False, right=False, bottom=False, which='both')\n",
    "    ax_inset_1.set_yticklabels([])\n",
    "    ax_inset_1.xaxis.set_major_formatter(mpl.ticker.NullFormatter())\n",
    "    ax_inset_1.xaxis.set_minor_formatter(mpl.ticker.NullFormatter())\n",
    "    ax_inset_1.set_title(r'$f_{T_1} = 0.1$', fontsize=10, color='white')\n",
    "\n",
    "    ax_inset_2 = ax[1].inset_axes([0.6,0.6,0.3,0.3])\n",
    "    ax_inset_2.pcolormesh(x, y, net_overheads_expansion_direct[:,:,2], norm=norm, cmap=cmap)\n",
    "    ax_inset_2.loglog()\n",
    "    ax_inset_2.tick_params(top=False, left=False, right=False, bottom=False, which='both')\n",
    "    ax_inset_2.set_yticklabels([])\n",
    "    ax_inset_2.xaxis.set_major_formatter(mpl.ticker.NullFormatter())\n",
    "    ax_inset_2.xaxis.set_minor_formatter(mpl.ticker.NullFormatter())\n",
    "    ax_inset_2.set_title(r'$f_{T_1} = 0.001$', fontsize=10, color='white')\n",
    "\n",
    "pcolormesh = ax[2].pcolormesh(x, y, net_overheads_expansion_scrambling, norm=norm, cmap=cmap)\n",
    "ax[2].set_title(r'$\\it{Code}$ $\\it{expansion}$ baseline' + '\\n' + r'($\\bf{Scrambling}$ noise model)')\n",
    "\n",
    "ax[3].pcolormesh(x, y, net_overheads_remap, norm=norm, cmap=cmap)\n",
    "ax[3].set_title('Re-mapping (ours)')\n",
    "ax[3].plot([3], [1/27/10*50e-3], 'x', color='k')\n",
    "ax[3].annotate('McEwen 2022', (3, 1/27/10*50e-3), textcoords='offset points', xytext=(5,5), ha='left', va='center', color='k')\n",
    "ax[3].plot([2.5], [1/27/16.4*1], 'x', color='k')\n",
    "ax[3].annotate('Thorbeck 2023', (2.5, 1/27/16.4*1), textcoords='offset points', xytext=(5,5), ha='left', va='center', color='k')\n",
    "ax[3].plot([1], [1/4/50*50e-3], 'x', color='k')\n",
    "ax[3].annotate('Wilen 2021', (1, 1/4/50*50e-3), textcoords='offset points', xytext=(5,5), ha='left', va='center', color='k')\n",
    "\n",
    "for i,axis in enumerate(ax):\n",
    "    axis.set_yscale('log')\n",
    "    # if i == 2:\n",
    "    #     axis.set_xticks([])\n",
    "    # else:\n",
    "    axis.set_xlabel(r'Ray radius $r_{\\text{CRE}}$')\n",
    "    axis.set_xticks([1,2,3,4])\n",
    "\n",
    "cbar = qc_utils.plot.add_cbar(ax[3], norm, cmap, size=0.01)\n",
    "# cbar = plt.colorbar(pcolormesh, ticks=[1, net_overheads_offline.max(), 4, 11,\n",
    "# 17])\n",
    "cbar.set_ticks([1,2,4,7,11,17])\n",
    "cbar.set_label('Relative qubitcycle cost per distillation')\n",
    "\n",
    "plt.savefig('../figures/ideal_detection_overheads.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min improvement over best other method (direct): 6.044950800666828\n",
      "Min improvement over best other method (scrambling): 6.044950800666828\n",
      "Max improvement over best other method (direct): 75641.08541172852\n",
      "Max improvement over best other method (scrambling): 75641.08541172852\n",
      "Geomean improvement over best other method (direct): 1276.2176545911981463812973542563126579565919284554\n",
      "Geomean improvement over best other method (scrambling): 1276.2176545911981463812973542563126579565919284554\n",
      "Min qubitcycle improvement over best other method (direct): 3.999625816644771\n",
      "Min qubitcycle improvement over best other method (scrambling): 3.999625816644771\n",
      "Max qubitcycle improvement over best other method (direct): 244.26533314963234\n",
      "Max qubitcycle improvement over best other method (scrambling): 244.26533314963234\n",
      "Geomean qubitcycle improvement over best other method (direct): 13.692556528370526598363610013707528083892894957756\n",
      "Geomean qubitcycle improvement over best other method (scrambling): 13.692556528370526598363610013707528083892894957756\n"
     ]
    }
   ],
   "source": [
    "def geo_mean(iterable):\n",
    "    a = np.array(iterable).flatten()\n",
    "    return a.prod()**(1.0/len(a))\n",
    "\n",
    "best_baseline_direct = np.minimum(net_overheads_distributed, net_overheads_expansion_direct[:,:,0])\n",
    "best_baseline_improvements_direct = (best_baseline_direct-1) / (net_overheads_remap-1)\n",
    "best_baseline_improvements_direct = best_baseline_improvements_direct[np.isfinite(best_baseline_improvements_direct)]\n",
    "best_baseline_scrambling = np.minimum(net_overheads_distributed, net_overheads_expansion_scrambling)\n",
    "best_baseline_improvements_scrambling = (best_baseline_direct-1) / (net_overheads_remap-1)\n",
    "best_baseline_improvements_scrambling = best_baseline_improvements_scrambling[np.isfinite(best_baseline_improvements_scrambling)]\n",
    "\n",
    "print('Min improvement over best other method (direct):', best_baseline_improvements_direct.min())\n",
    "print('Min improvement over best other method (scrambling):', best_baseline_improvements_scrambling.min())\n",
    "\n",
    "print('Max improvement over best other method (direct):', best_baseline_improvements_direct.max())\n",
    "print('Max improvement over best other method (scrambling):', best_baseline_improvements_scrambling.max())\n",
    "\n",
    "print('Geomean improvement over best other method (direct):', geo_mean([mpmath.mpf(x) for x in best_baseline_improvements_direct]))\n",
    "print('Geomean improvement over best other method (scrambling):', geo_mean([mpmath.mpf(x) for x in best_baseline_improvements_scrambling]))\n",
    "\n",
    "best_baseline_qubitcycle_improvements_direct = best_baseline_direct / net_overheads_remap\n",
    "best_baseline_qubitcycle_improvements_direct = best_baseline_qubitcycle_improvements_direct[np.isfinite(best_baseline_qubitcycle_improvements_direct)]\n",
    "best_baseline_qubitcycle_improvements_scrambling = best_baseline_direct / net_overheads_remap\n",
    "best_baseline_qubitcycle_improvements_scrambling = best_baseline_qubitcycle_improvements_scrambling[np.isfinite(best_baseline_qubitcycle_improvements_scrambling)]\n",
    "\n",
    "print('Min qubitcycle improvement over best other method (direct):', best_baseline_qubitcycle_improvements_direct.min())\n",
    "print('Min qubitcycle improvement over best other method (scrambling):', best_baseline_qubitcycle_improvements_scrambling.min())\n",
    "\n",
    "print('Max qubitcycle improvement over best other method (direct):', best_baseline_qubitcycle_improvements_direct.max())\n",
    "print('Max qubitcycle improvement over best other method (scrambling):', best_baseline_qubitcycle_improvements_scrambling.max())\n",
    "\n",
    "print('Geomean qubitcycle improvement over best other method (direct):', geo_mean([mpmath.mpf(x) for x in best_baseline_qubitcycle_improvements_direct]))\n",
    "print('Geomean qubitcycle improvement over best other method (scrambling):', geo_mean([mpmath.mpf(x) for x in best_baseline_qubitcycle_improvements_scrambling]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7143601371065397e-08"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.poisson.pmf(2, 1/27/10*50e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting $\\Gamma$ and $T_{\\text{offline}}$ for our  method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding max and average qubitcycle improvement numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar plot for specific parameter settings (e.g. Google Direct, Scrambling (fast recal), Scrambling (slow recal), ...|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02438274780070832"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.poisson.pmf(1, 25e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-delay-mG6OZh6y-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
